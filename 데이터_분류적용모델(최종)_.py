# -*- coding: utf-8 -*-
"""데이터_분류적용모델(최종) .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USUrgtnEsj9PXzk8u4c1psi3WGIU_tHZ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# 한글 인코딩
from matplotlib import font_manager, rc
import platform
import matplotlib
# %matplotlib inline 

if platform.system() == 'Windows':
    font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
    rc('font', family = font_name)
else:
    rc('font',family='AppleGothic')
    
matplotlib.rcParams['axes.unicode_minus']=False
# %matplotlib inline

#데이터 불러오기
#침수된 데이터가 침수되지 데이터에 비해 현저히 적은 것을 알 수 있음
data = pd.read_excel('/content/MZ세대 행정동별 데이터셋 (최종2).xlsx')

data = data.iloc[:, :]
# data.head()

#data.columns


#Create labels and features
# 세대당인구, 가구소득 300만원 미만, 가구소득 300~600만원 미만, Z세대사업체수비율, Z세대종사자수비율, 사업체신규비율, 사업체이탈비율 상태변수 제거
X_numerical = data.drop(data.columns[[0, 1]], axis=1).astype('float64')
X_numerical = data.drop(['MZ세대 평균 소비 금액', '행정동', '세대당인구', '가구소득 300만원 미만', '가구소득 300~600만원 미만', 'Z세대사업체수비율', 'Z세대종사자수비율', '사업체신규비율', '사업체이탈비율'], axis=1).round(2)

list_numerical = X_numerical.columns
#list_numerical

# Create all features
X = data.iloc[:, 2:]
X = X.drop(['세대당인구', '가구소득 300만원 미만', '가구소득 300~600만원 미만', 'Z세대사업체수비율', 'Z세대종사자수비율', '사업체신규비율', '사업체이탈비율'], axis=1).round(2)

y = data['MZ세대 평균 소비 금액']


# Split data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)

# Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train[list_numerical]) 
X_train[list_numerical] = scaler.transform(X_train[list_numerical])
X_test[list_numerical] = scaler.transform(X_test[list_numerical])


# Ridge regression
from sklearn.linear_model import Ridge, Lasso, ElasticNet
model = Lasso(alpha=1)
results = model.fit(X_train, y_train)


# 후진 제거법 (Backward Elimination)
# feature selection의 읠환으로 변수를 줄인다
# 모든 독립변수(설명변수)를 고려한 모델에서 유의하지 않은 설명변수를 하나씩 제거하는 방법
# => 모든 변수를 입력한 뒤 가장 유의학률이 큰 설명 변수를 제외
# 모든 설명변수가 유의하다 판정될때 까지 반복
from sklearn.feature_selection import RFE # 반복적 변수 제거를 하여 변수별 중요도를 도출
rfe = RFE(model, n_features_to_select=13, step=1) # 골라낼 변수의 수, step: 한번에 몇개씩 제거할지 선택
model2 = rfe.fit(X, y)
print('Support :', model2.support_)         # 선택된 변수
print('Ranking :', model2.ranking_)         # 변수 중요도(숫자 높을수록 불필요)


# Model evaluation
# R^2-score
print('R squared training set', round(model.score(X_train, y_train)*1, 2)) # 1에 가까울 수록 좋은 성능
print('R squared test set', round(model.score(X_test, y_test)*1, 2))

# Training data
from sklearn.metrics import mean_squared_error
pred_train = model.predict(X_train) # 0에 가까울 수록 좋은 성능
mse_train = mean_squared_error(y_train, pred_train)
print('MSE training set', round(mse_train, 5))

# Test data
pred = model.predict(X_test)
mse_test =mean_squared_error(y_test, pred)
print('MSE :', round(mse_test, 5))


#예측 모델 생성
y_p = model.predict(X_test)

#print(X_train, y_train)
print('Accuracy :', round(model.score(X_test, y_test), 4)) # 예측 정확도
relation_square = model.score(X_test, y_test) #결정계수
print('결정계수 :', relation_square)
print('y절편 : ', model.intercept_)

print(model.coef_)


param = model.coef_
#for p in range(len(param)):
     #param[p] = abs(param[p])
plt.bar(X_train.columns, param, label='weight')
plt.title('MZ세대 평균 소비 금액', fontsize=15)
plt.xticks(X_train.columns, X_train.columns.tolist(), rotation='vertical')
plt.legend()
plt.show()

import seaborn as sns
ax1 = sns.distplot(y, hist = False, label = 'y실제')
ax2 = sns.distplot(y_p, hist = False, label = 'y예측')
plt.legend()

# 최종 상태변수 선정
X_numerical = data.drop(['MZ세대 평균 소비 금액', '행정동', '세대당인구', '가구소득 300만원 미만', '가구소득 300~600만원 미만', 'Z세대사업체수비율', 'Z세대종사자수비율', '사업체신규비율', '사업체이탈비율'], axis=1).round(2)
X_numerical

import matplotlib.pyplot as plt
plt.plot( y_train, 'k.')
plt.plot(y_p)
plt.xlabel("Actual Rent")
plt.ylabel("Predicted Rent")
plt.title("MULTIPLE LINEAR REGRESSION")